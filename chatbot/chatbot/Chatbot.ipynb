{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41476bf8-312c-4d75-8090-888c0beda60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec040a3-3bb7-43d7-b2b1-6452b3b7f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q streamlit langchain langchain-openai langchain-community pypdf faiss-cpu openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05929e12-6ba6-46c8-96f2-4ede678e9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"app.py\"\n",
    "# Importamos las librer칤as necesarias para la aplicaci칩n\n",
    "import streamlit as st  # Para crear la interfaz web de manera sencilla\n",
    "import os  # Para interactuar con el sistema operativo (rutas de archivos, variables de entorno)\n",
    "\n",
    "# --- IMPORTACIONES ESENCIALES Y ROBUSTAS ---\n",
    "# Estas son las herramientas espec칤ficas de LangChain que usaremos\n",
    "from langchain_community.document_loaders import PyPDFLoader  # Para leer el contenido de archivos PDF\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Para dividir textos largos en fragmentos m치s peque침os y manejables\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # Para usar los modelos de OpenAI (embeddings para vectores y ChatOpenAI para el bot)\n",
    "from langchain_community.vectorstores import FAISS  # Base de datos vectorial eficiente para buscar informaci칩n similar\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Para crear plantillas de prompts estructurados para el chat\n",
    "from langchain_core.messages import AIMessage, HumanMessage  # Para representar los mensajes del bot (AI) y del usuario (Human) en el historial\n",
    "\n",
    "# Configuramos la p치gina de Streamlit \n",
    "st.set_page_config(page_icon=\"游볟\", page_title=\"Asistente Silver Economy\")\n",
    "st.title(\"游볟 Asistente Silver Economy (Funcional)\")\n",
    "\n",
    "# --- CONFIGURACI칍N DE LA API KEY DE OPENAI ---\n",
    "# Verificamos si la clave ya est치 en las variables de entorno del sistema\n",
    "if \"OPENAI_API_KEY\" in os.environ:\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "else:\n",
    "    # Si no est치, creamos un campo de entrada de contrase침a en la barra lateral para que el usuario la ingrese\n",
    "    openai_api_key = st.sidebar.text_input(\"OpenAI API Key:\", type=\"password\")\n",
    "    # Si el usuario a칰n no ha ingresado la clave, mostramos una advertencia y detenemos la ejecuci칩n\n",
    "    if not openai_api_key:\n",
    "        st.warning(\"Introduce tu API Key para continuar.\")\n",
    "        st.stop()\n",
    "    # Una vez ingresada, la guardamos en las variables de entorno para que LangChain la use autom치ticamente\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# --- FUNCI칍N PARA CARGAR Y PROCESAR LOS DATOS ---\n",
    "# Usamos @st.cache_resource para que esta funci칩n pesada solo se ejecute una vez y se guarde en memoria\n",
    "@st.cache_resource\n",
    "def iniciar_base_datos():\n",
    "    # Definimos la ruta donde est치n los archivos PDF. \"./Data\" significa la carpeta 'Data' en el directorio actual.\n",
    "    ruta_data = \"./Data\"\n",
    "    # Verificamos que la carpeta exista para evitar errores\n",
    "    if not os.path.exists(ruta_data):\n",
    "        st.error(\"No se encuentra la carpeta 'Data'. Aseg칰rate de ejecutar esto desde la carpeta 'Tarea 4'.\")\n",
    "        st.stop()\n",
    "    \n",
    "    docs = []  # Lista para guardar todos los documentos cargados\n",
    "    # Mostramos un spinner mientras se cargan los archivos para dar feedback al usuario\n",
    "    with st.spinner(\"Cargando documentos...\"):\n",
    "        # Recorremos todos los archivos en la carpeta 'Data'\n",
    "        for archivo in os.listdir(ruta_data):\n",
    "            # Si el archivo es un PDF, lo cargamos\n",
    "            if archivo.endswith(\".pdf\"):\n",
    "                # Creamos la ruta completa al archivo\n",
    "                ruta_completa = os.path.join(ruta_data, archivo)\n",
    "                # Usamos PyPDFLoader para leer el PDF\n",
    "                loader = PyPDFLoader(ruta_completa)\n",
    "                # A침adimos el contenido del PDF a nuestra lista de documentos\n",
    "                docs.extend(loader.load())\n",
    "    \n",
    "    # Dividimos los documentos en fragmentos (chunks) m치s peque침os\n",
    "    # chunk_size=1000: cada fragmento tendr치 aprox. 1000 caracteres\n",
    "    # chunk_overlap=200: los fragmentos se solapar치n en 200 caracteres para no perder contexto entre cortes\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Creamos la base de datos vectorial (FAISS) usando los embeddings de OpenAI\n",
    "    # Esto convierte los fragmentos de texto en vectores num칠ricos para poder buscar por similitud sem치ntica\n",
    "    return FAISS.from_documents(splits, OpenAIEmbeddings())\n",
    "\n",
    "# --- INICIALIZACI칍N DEL ESTADO DE LA APLICACI칍N ---\n",
    "# Verificamos si la base de datos vectorial ya est치 cargada en la sesi칩n\n",
    "if \"vectorstore\" not in st.session_state:\n",
    "    # Si no est치, llamamos a la funci칩n para cargarla\n",
    "    st.session_state.vectorstore = iniciar_base_datos()\n",
    "    # Creamos un 'retriever' (buscador) a partir de la base vectorial\n",
    "    # search_kwargs={\"k\": 3} significa que buscar치 los 3 fragmentos m치s relevantes para cada pregunta\n",
    "    st.session_state.retriever = st.session_state.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    st.success(\"춰Sistema listo! Haz tu pregunta.\")\n",
    "\n",
    "# --- INICIALIZACI칍N DEL HISTORIAL DEL CHAT ---\n",
    "# Si no existe el historial en la sesi칩n, creamos una lista vac칤a para guardarlo\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# --- CONFIGURACI칍N DEL MOTOR DEL CHAT (MODELO DE LENGUAJE) ---\n",
    "# Usamos el modelo GPT-3.5 Turbo con temperatura 0 para respuestas m치s precisas y menos creativas (ideal para RAG)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# --- DEFINICI칍N DE LA PLANTILLA DEL PROMPT ---\n",
    "# Esta plantilla le dice al modelo c칩mo debe comportarse y qu칠 formato usar\n",
    "template = \"\"\"Eres un asistente 칰til para personas mayores. Basate SOLO en el siguiente contexto para responder.\n",
    "Si no lo sabes, di que no tienes esa informaci칩n en tus documentos.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Historial:\n",
    "{chat_history}\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta:\"\"\"\n",
    "\n",
    "# Creamos el objeto PromptTemplate a partir del texto de la plantilla\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# --- FUNCI칍N RAG MANUAL (Retrieval-Augmented Generation) ---\n",
    "# Esta funci칩n orquesta todo el proceso de respuesta\n",
    "def responder_pregunta(pregunta, historial):\n",
    "    # 1. RECUPERACI칍N (Retrieval): Buscamos los documentos m치s relevantes para la pregunta en nuestra base vectorial\n",
    "    docs_relevantes = st.session_state.retriever.invoke(pregunta)\n",
    "    # Unimos el contenido de los documentos recuperados en un solo texto\n",
    "    contexto_texto = \"\\n\\n\".join([d.page_content for d in docs_relevantes])\n",
    "    \n",
    "    # 2. FORMATEO DEL HISTORIAL: Convertimos la lista de mensajes en un texto simple para el prompt\n",
    "    # Tomamos solo los 칰ltimos 4 mensajes para no sobrecargar el contexto\n",
    "    historial_texto = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in historial[-4:]])\n",
    "    \n",
    "    # 3. GENERACI칍N DEL PROMPT: Rellenamos la plantilla con el contexto, historial y la pregunta actual\n",
    "    prompt_final = prompt_template.format_messages(\n",
    "        context=contexto_texto,\n",
    "        chat_history=historial_texto,\n",
    "        question=pregunta\n",
    "    )\n",
    "    \n",
    "    # 4. LLAMADA AL MODELO: Enviamos el prompt completo al LLM y obtenemos su respuesta\n",
    "    respuesta_llm = llm.invoke(prompt_final)\n",
    "    # Devolvemos solo el contenido de texto de la respuesta\n",
    "    return respuesta_llm.content\n",
    "\n",
    "# --- INTERFAZ DE USUARIO (CHAT) ---\n",
    "# Mostramos todos los mensajes que ya est치n en el historial\n",
    "for msg in st.session_state.chat_history:\n",
    "    # Determinamos si el mensaje es del asistente o del usuario para mostrarlo correctamente\n",
    "    tipo = \"assistant\" if isinstance(msg, AIMessage) else \"user\"\n",
    "    with st.chat_message(tipo):\n",
    "        st.markdown(msg.content)\n",
    "\n",
    "# Capturamos la nueva pregunta del usuario desde el campo de entrada\n",
    "if user_query := st.chat_input(\"Escribe aqu칤...\"):\n",
    "    # Agregamos la pregunta del usuario al historial y la mostramos en el chat\n",
    "    st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_query)\n",
    "    \n",
    "    # Generamos la respuesta del asistente y la mostramos\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Consultando documentos...\"):\n",
    "            # Llamamos a nuestra funci칩n RAG manual\n",
    "            respuesta = responder_pregunta(user_query, st.session_state.chat_history)\n",
    "            st.markdown(respuesta)\n",
    "    \n",
    "    # Agregamos la respuesta del asistente al historial para mantener la conversaci칩n\n",
    "    st.session_state.chat_history.append(AIMessage(content=respuesta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034dd372-179e-434b-bee0-03fa1aa1dec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
